# -*- coding: utf-8 -*-
"""HackathonNotebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rk8Z4umcP5prNt5H9YPmgE5wS7OhN3S

Some useful intro text!
"""

https://data.usatoday.com/climate-data/adams-county-pennsylvania/42001/1961-01-01/?syear=1895&eyear=2023

"""#Imports"""

!pip install wandb --quiet

import torch
from torchsummary import summary
import torchvision #This library is used for image-based operations (Augmentations)
import os
import gc
from tqdm import tqdm
from PIL import Image
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
import glob
import wandb
import matplotlib.pyplot as plt
# from torchinfo import summary # (or this if torchsummary sucks (sometimes breaks in current version))
import torch.nn.functional as F
import torch.nn as nn

"""# GPU Check"""

!nvidia-smi # to see what GPU you have

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Device: ", DEVICE)

"""# Configurations / Custom Modules"""

config = {
    'batch_size': 64,           # This is way too small. Make it larger depending
    'lr': 0.01,
    'epochs': 25,
    'weight_decay' : 0.0001,
    'dropout' : 0.3             # Try 0.3-0.5 for MLP. For CNN/RNN try 0.1 or lower (custom structure.)
}

"""## Activations"""

class Swish(torch.nn.Module):
    def __init__(self, learnable=True):
        super(Swish, self).__init__()
        # If learnable is True, beta is a learnable parameter, otherwise, it is fixed to 1
        self.beta = torch.nn.Parameter(torch.ones(1)) if learnable else torch.tensor(1.0)

    def forward(self, input):
        return input * torch.sigmoid(self.beta * input)

class Serf(torch.nn.Module):
    def __init__(self):
        super(Serf, self).__init__()

    def forward(self, x):
        return x * torch.erf(F.softplus(x))

"""# Dataset Processing/Download"""

!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8
!mkdir /root/.kaggle

with open("/root/.kaggle/kaggle.json", "w+") as f:
    f.write('{"username":"alexandermoker","key":"e731dead02aa5b233e613af25dd23016"}')
    # Put your kaggle username & key here

!chmod 600 /root/.kaggle/kaggle.json

def calculate_normalized_danger_score_for_weather(F, P):
    """
    Calculate the normalized danger score for given conditions.

    Parameters:
    - F: Temperature in degrees Fahrenheit.
    - P: Precipitation as a percentage (e.g., 0.5 for 50%).
    - V: Visibility in kilometers.

    Returns:
    - Normalized danger score, scaled to a maximum of 5.
    """
    # Constants
    A = 100  # Scale factor for temperature and precipitation impact
    B = 10   # Temperature sensitivity adjustment
    C = 90   # Base level precipitation adjustment
    D = 50   # Scale factor for visibility impact
    E = 5    # Visibility sensitivity adjustment
    N = 100  # Normalization factor, based on a hypothetical maximum danger score of 500

    # Convert Fahrenheit to Celsius for the calculation
    T_celsius = (F - 32) / 1.8

    # Calculate the temperature and precipitation component
    temp_precip_component = (A / (T_celsius + B)) * (P+C)

    # Calculate the visibility component
    #visibility_component = D / (V + E)

    # Calculate the raw danger score
    raw_danger_score = temp_precip_component # * visibility_component

    # Normalize the danger score
    normalized_danger_score = raw_danger_score / N

    return normalized_danger_score

def calculate_danger_score_for_time(expected_time):
    """
    Calculates a danger score for a given expected time based on an exponential scaling of time.

    Parameters:
    - expected_time: Expected time in minutes.

    Returns:
    - Danger score for the given expected time.
    """
    # Convert expected time to a fraction of an hour
    x = expected_time / 60.0

    # Apply the function x^e, where e is the base of the natural logarithm
    danger_score = np.power(x, np.e)

    return danger_score

def calculate_danger_score_for_traffic(traffic):
    """
    Calculates a danger score for a given traffic level based a linear scaling traffic danger value.

    Parameters:
    - traffic: Traffic

    Returns:
    - Danger score for the given traffic level.
    """

    danger_score = 0

    # Convert traffic to a danger score
    if traffic == 'light':
        danger_score = 1.0
    elif traffic == 'moderate':
        danger_score = 2.0
    elif traffic == 'heavy':
        danger_score = 3.0

    # Normalizes score
    return (danger_score / 3.0)

def calculate_danger_score(weather_danger_score, time_danger_score):
    """
    Calculates a danger score for a given traffic, weather, and time

    Parameters:
    - traffic_danger_score: Danger score for traffic
    - weather_danger_score: Danger score for weather
    - time_danger_score: Danger score for time

    Returns:
    - Danger score for the given traffic, weather, and time
    """

    # Combine the individual danger scores
    combined_danger_score = time_danger_score + weather_danger_score # + traffic_danger_score

    # Apply the day / night multiplier
    #day_night_multiplier = calculate_day_night_multiplier(is_night)
    adjusted_danger_score = combined_danger_score # * day_night_multiplier

    return adjusted_danger_score

"""#Dataset Generation"""

# All cities in PA
cities = ['Aaronsburg', 'Abbottstown', 'Adamsburg', 'Adamstown', 'Addison', 'Akron', 'Albion', 'Alburtis', 'Alexandria', 'Aliquippa', 'Allenport', 'Allentown', 'Altoona', 'Ambler', 'Ambridge', 'Amity', 'Annville', 'Apollo', 'Archbald', 'Ardmore', 'Arendtsville', 'Aristes', 'Arona', 'Ashland', 'Ashville', 'Atglen', 'Auburn', 'Audubon', 'Austin', 'Avis', 'Avondale', 'Avonmore', 'Baden', 'Bally', 'Bangor', 'Bath', 'Beallsville', 'Bear Creek', 'Beaver Falls', 'Beaver Meadows', 'Beaver Springs', 'Beaver', 'Beaverdale', 'Beavertown', 'Bechtelsville', 'Beech Creek', 'Belle Vernon', 'Bellefonte', 'Belleville', 'Bellwood', 'Bendersville', 'Bentleyville', 'Benton', 'Berlin', 'Bernville', 'Berrysburg', 'Berwick', 'Bessemer', 'Bethel Park', 'Bethlehem', 'Big Run', 'Biglerville', 'Birdsboro', 'Black Lick', 'Blain', 'Blairsville', 'Blanchard', 'Blooming Glen', 'Bloomsburg', 'Blossburg', 'Blue Bell', 'Boalsburg', 'Boiling Springs', 'Bolivar', 'Boswell', 'Bowmanstown', 'Boyertown', 'Brackenridge', 'Braddock', 'Bradford', 'Bradfordwoods', 'Branchdale', 'Bridgeport', 'Brisbin', 'Bristol', 'Broad Top', 'Brockway', 'Brodheadsville', 'Brookhaven', 'Brookville', 'Broomall', 'Brownstown', 'Brownsville', 'Bruin', 'Bryn Athyn', 'Bryn Mawr', 'Burgettstown', 'Burnham', 'Burnside', 'Butler', 'California', 'Callensburg', 'Callery', 'Calumet', 'Cambridge Springs', 'Camp Hill', 'Campbelltown', 'Canonsburg', 'Canton', 'Carbondale', 'Carlisle', 'Carmichaels', 'Carnegie', 'Carrolltown', 'Cashtown', 'Catasauqua', 'Catawissa', 'Cecil', 'Centerport', 'Centerville', 'Central City', 'Centre Hall', 'Chalfont', 'Chambersburg', 'Charleroi', 'Cherry Tree', 'Chester Heights', 'Chester', 'Chicora', 'Christiana', 'Clairton', 'Clarence', 'Clarendon', 'Clarion', 'Clark', 'Clarks Summit', 'Clarksville', 'Claysburg', 'Claysville', 'Clearfield', 'Clifton Heights', 'Clintonville', 'Clymer', 'Coaldale', 'Coalport', 'Coatesville', 'Cochranton', 'Cokeburg', 'Collegeville', 'Columbia', 'Colver', 'Commodore', 'Confluence', 'Conneaut Lake', 'Conneautville', 'Connellsville', 'Connoquenessing', 'Conshohocken', 'Conway', 'Conyngham', 'Coopersburg', 'Cooperstown', 'Coplay', 'Coraopolis', 'Cornwall', 'Corry', 'Corsica', 'Coudersport', 'Crabtree', 'Cranesville', 'Creekside', 'Cresson', 'Cressona', 'Croydon', 'Curtisville', 'Curwensville', 'Daisytown', 'Dallas', 'Dallastown', 'Dalton', 'Danville', 'Darby', 'Darlington', 'Dauphin', 'Davidsville', 'Dawson', 'Dayton', 'Delano', 'Delaware Water Gap', 'Delmont', 'Delta', 'Denver', 'Derry', 'Dickson City', 'Dillsburg', 'Donora', 'Dover', 'Doylestown', 'Dravosburg', 'Drexel Hill', 'Du Bois', 'Dublin', 'Dunbar', 'Duncannon', 'Duncansville', 'Dunlevy', 'Duquesne', 'Duryea', 'Dushore', 'Eagleville', 'East Berlin', 'East Brady', 'East Butler', 'East Greenville', 'East Mc Keesport', 'East Petersburg', 'East Pittsburgh', 'East Prospect', 'East Stroudsburg', 'East Vandergrift', 'Easton', 'Eau Claire', 'Ebensburg', 'Edinboro', 'Elco', 'Elderton', 'Eldred', 'Elgin', 'Elizabeth', 'Elizabethtown', 'Elizabethville', 'Elkland', 'Ellsworth', 'Ellwood City', 'Elysburg', 'Emigsville', 'Emlenton', 'Emmaus', 'Emporium', 'Enola', 'Enon Valley', 'Ephrata', 'Erie', 'Ernest', 'Evans City', 'Everett', 'Everson', 'Export', 'Exton', 'Factoryville', 'Fairchance', 'Fairfield', 'Fairless Hills', 'Fairview', 'Falls Creek', 'Farrell', 'Fawn Grove', 'Fayette City', 'Fayetteville', 'Feasterville Trevose', 'Felton', 'Ferndale', 'Finleyville', 'Fleetwood', 'Flourtown', 'Folcroft', 'Folsom', 'Ford City', 'Ford Cliff', 'Forest City', 'Forest Grove', 'Forestville', 'Fort Washington', 'Foxburg', 'Frackville', 'Franklin', 'Franklintown', 'Fredericksburg', 'Fredericktown', 'Fredonia', 'Freeburg', 'Freedom', 'Freeland', 'Freeport', 'Friedens', 'Friedensburg', 'Galeton', 'Gallitzin', 'Gap', 'Garrett', 'Gastonville', 'Gettysburg', 'Gilberton', 'Gilbertsville', 'Girard', 'Glassport', 'Glen Campbell', 'Glen Lyon', 'Glen Rock', 'Glenolden', 'Glenside', 'Gordon', 'Grampian', 'Grapeville', 'Gratz', 'Great Bend', 'Green Lane', 'Greensburg', 'Greenville', 'Grindstone', 'Grove City', 'Halifax', 'Hallstead', 'Hamburg', 'Hanover', 'Harleysville', 'Harmonsburg', 'Harmony', 'Harrisburg', 'Harrisville', 'Hartleton', 'Hartstown', 'Harveys Lake', 'Hastings', 'Hatboro', 'Hatfield', 'Hawley', 'Hawthorn', 'Hazleton', 'Heilwood', 'Hellertown', 'Herminie', 'Hermitage', 'Herndon', 'Hershey', 'Highspire', 'Hiller', 'Hollidaysburg', 'Homer City', 'Homestead', 'Honesdale', 'Honey Brook', 'Hooversville', 'Hop Bottom', 'Hopewell', 'Hopwood', 'Horsham', 'Houston', 'Houtzdale', 'Howard', 'Hughesville', 'Hummels Wharf', 'Hummelstown', 'Hunker', 'Huntingdon', 'Hyde Park', 'Hyde', 'Hydetown', 'Hyndman', 'Imperial', 'Jackson Center', 'Jackson', 'Jamestown', 'Jeannette', 'Jefferson', 'Jenkintown', 'Jennerstown', 'Jermyn', 'Jerome', 'Jersey Shore', 'Jessup', 'Jim Thorpe', 'Johnsonburg', 'Johnstown', 'Jonestown', 'Kane', 'King Of Prussia', 'Kingston', 'Kittanning', 'Knox', 'Knoxville', 'Koppel', 'Kreamer', 'Kulpmont', 'Kulpsville', 'Kutztown', 'Laceyville', 'Lake City', 'Lancaster', 'Lanesboro', 'Langhorne', 'Lansdale', 'Lansdowne', 'Lansford', 'Laporte', 'Latrobe', 'Lavelle', 'Lawrenceville', 'Lawton', 'Le Raysville', 'Lebanon', 'Leechburg', 'Leesport', 'Leetsdale', 'Lehighton', 'Lemont', 'Lemoyne', 'Levittown', 'Lewis Run', 'Lewisberry', 'Lewisburg', 'Lewistown', 'Liberty', 'Light Street', 'Ligonier', 'Lilly', 'Linesville', 'Lionville', 'Lititz', 'Little Meadows', 'Littlestown', 'Liverpool', 'Lock Haven', 'Loganton', 'Loganville', 'Loretto', 'Lucernemines', 'Luzerne', 'Lykens', 'Lyon Station', 'Macungie', 'Madison', 'Mahaffey', 'Mahanoy City', 'Malvern', 'Manchester', 'Manheim', 'Manns Choice', 'Manor', 'Mansfield', 'Mapleton Depot', 'Marcus Hook', 'Marianna', 'Marietta', 'Marion Center', 'Marion Heights', 'Markleysburg', 'Mars', 'Martin', 'Martinsburg', 'Marysville', 'Masontown', 'Matamoras', 'Maytown', 'Mc Alisterville', 'Mc Clure', 'Mc Connellsburg', 'Mc Donald', 'Mc Ewensville', 'Mc Kean', 'Mc Kees Rocks', 'Mc Keesport', 'Mc Sherrystown', 'Mc Veytown', 'Meadville', 'Mechanicsburg', 'Mechanicsville', 'Media', 'Mercer', 'Mercersburg', 'Meshoppen', 'Mexico', 'Meyersdale', 'Middleburg', 'Middleport', 'Middletown', 'Midland', 'Midway', 'Mifflin', 'Mifflinburg', 'Mifflintown', 'Mifflinville', 'Milesburg', 'Milford', 'Mill Creek', 'Mill Hall', 'Mill Village', 'Millersburg', 'Millerstown', 'Millersville', 'Millheim', 'Millville', 'Milroy', 'Milton', 'Minersville', 'Modena', 'Mohnton', 'Monaca', 'Monessen', 'Monongahela', 'Monroeville', 'Mont Alto', 'Montgomery', 'Montgomeryville', 'Montoursville', 'Montrose', 'Moosic', 'Morrisville', 'Morton', 'Moscow', 'Mount Carmel', 'Mount Gretna', 'Mount Holly Springs', 'Mount Jewett', 'Mount Joy', 'Mount Pleasant Mills', 'Mount Pleasant', 'Mount Pocono', 'Mount Union', 'Mount Wolf', 'Mountainhome', 'Mountville', 'Muncy', 'Myerstown', 'Nanticoke', 'Nanty Glo', 'Narberth', 'Nazareth', 'Nemacolin', 'Nescopeck', 'Nesquehoning', 'New Albany', 'New Alexandria', 'New Berlin', 'New Bethlehem', 'New Brighton', 'New Castle', 'New Columbia', 'New Cumberland', 'New Eagle', 'New Florence', 'New Freedom', 'New Galilee', 'New Holland', 'New Hope', 'New Kensington', 'New Kingstown', 'New Milford', 'New Oxford', 'New Paris', 'New Philadelphia', 'New Ringgold', 'New Salem', 'New Stanton', 'New Wilmington', 'Newburg', 'Newell', 'Newmanstown', 'Newport', 'Newry', 'Newton Hamilton', 'Newtown', 'Newville', 'Nicholson', 'Norristown', 'North Apollo', 'North East', 'North Versailles', 'North Wales', 'Northampton', 'Northern Cambria', 'Northumberland', 'Norwood', 'Numidia', 'Nuremberg', 'Oakdale', 'Oakmont', 'Oil City', 'Old Forge', 'Oliver', 'Olyphant', 'Oneida', 'Orangeville', 'Orbisonia', 'Oreland', 'Orrstown', 'Orwigsburg', 'Osceola Mills', 'Oxford', 'Palmerton', 'Palmyra', 'Paoli', 'Paradise', 'Parker', 'Parkesburg', 'Parryville', 'Patton', 'Paxtonville', 'Pen Argyl', 'Penn', 'Penns Creek', 'Pennsburg', 'Perkasie', 'Perryopolis', 'Petersburg', 'Petrolia', 'Philadelphia', 'Philipsburg', 'Phoenixville', 'Picture Rocks', 'Pillow', 'Pine Grove Mills', 'Pine Grove', 'Pitcairn', 'Pittsburgh', 'Pittston', 'Plainfield', 'Pleasantville', 'Plumville', 'Plymouth Meeting', 'Plymouth', 'Pocono Pines', 'Point Marion', 'Polk', 'Port Allegany', 'Port Clinton', 'Port Matilda', 'Port Royal', 'Port Trevorton', 'Portage', 'Portersville', 'Portland', 'Potts Grove', 'Pottstown', 'Pottsville', 'Prompton', 'Prospect Park', 'Prospect', 'Punxsutawney', 'Quakertown', 'Quarryville', 'Quentin', 'Railroad', 'Ramey', 'Ravine', 'Reading', 'Reamstown', 'Rebersburg', 'Red Hill', 'Red Lion', 'Reedsville', 'Renovo', 'Republic', 'Reynoldsville', 'Rheems', 'Rices Landing', 'Richboro', 'Richfield', 'Richland', 'Richlandtown', 'Ridgway', 'Ridley Park', 'Riegelsville', 'Rimersburg', 'Ringtown', 'Riverside', 'Roaring Spring', 'Robesonia', 'Robinson', 'Rochester', 'Rockhill Furnace', 'Rockwood', 'Rome', 'Roscoe', 'Rossiter', 'Rouseville', 'Rouzerville', 'Royersford', 'Rural Valley', 'Russellton', 'Saint Clair', 'Saint Marys', 'Saint Michael', 'Saint Petersburg', 'Salisbury', 'Saltillo', 'Saltsburg', 'Sandy Lake', 'Sandy Ridge', 'Saxonburg', 'Sayre', 'Schaefferstown', 'Schellsburg', 'Schnecksville', 'Schuylkill Haven', 'Schwenksville', 'Scottdale', 'Scranton', 'Selinsgrove', 'Sellersville', 'Seltzer', 'Seneca', 'Seven Valleys', 'Seward', 'Sewickley', 'Shamokin Dam', 'Shamokin', 'Shanksville', 'Sharon Hill', 'Sharon', 'Sharpsville', 'Sheffield', 'Shenandoah', 'Sheppton', 'Shickshinny', 'Shippenville', 'Shippingport', 'Shoemakersville', 'Silverdale', 'Skippack', 'Slatington', 'Slickville', 'Sligo', 'Slippery Rock', 'Smethport', 'Smithfield', 'Smithton', 'Snow Shoe', 'Snydertown', 'Somerset', 'Souderton', 'South Fork', 'South Heights', 'South Park', 'Southwest', 'Spartansburg', 'Spring City', 'Spring Grove', 'Spring House', 'Spring Mills', 'Spring Mount', 'Springboro', 'Springdale', 'Springfield', 'Starrucca', 'State College', 'Stewartstown', 'Stockdale', 'Stockertown', 'Stoneboro', 'Stoystown', 'Strasburg', 'Strattanville', 'Strausstown', 'Stroudsburg', 'Sturgeon', 'Sugar Grove', 'Summerhill', 'Summerville', 'Summit Hill', 'Summit Station', 'Sunbury', 'Susquehanna', 'Sutersville', 'Swarthmore', 'Sykesville', 'Tamaqua', 'Tarentum', 'Tatamy', 'Taylor', 'Telford', 'Terre Hill', 'Thompson', 'Thompsontown', 'Three Springs', 'Tidioute', 'Tioga', 'Tionesta', 'Tipton', 'Titusville', 'Topton', 'Toughkenamon', 'Towanda', 'Tower City', 'Townville', 'Trafford', 'Tremont', 'Tresckow', 'Trevorton', 'Troutville', 'Troy', 'Trumbauersville', 'Tunkhannock', 'Turbotville', 'Turtle Creek', 'Tuscarora', 'Tyrone', 'Ulysses', 'Union City', 'Union Dale', 'Uniontown', 'Ursina', 'Utica', 'Valencia', 'Valley View', 'Vanderbilt', 'Vandergrift', 'Venango', 'Verona', 'Vintondale', 'Wallaceton', 'Walnutport', 'Wampum', 'Warminster', 'Warren', 'Washington', 'Washingtonville', 'Waterford', 'Watsontown', 'Wattsburg', 'Waymart', 'Wayne', 'Waynesboro', 'Waynesburg', 'Weatherly', 'Wellsboro', 'Wellsville', 'Wernersville', 'West Alexander', 'West Chester', 'West Elizabeth', 'West Grove', 'West Middlesex', 'West Mifflin', 'West Newton', 'Westfield', 'Westover', 'Wheatland', 'White Haven', 'White', 'Whitehall', 'Wilkes Barre', 'Williamsburg', 'Williamsport', 'Williamstown', 'Willow Grove', 'Willow Street', 'Wilmerding', 'Wilmore', 'Wind Gap', 'Windber', 'Windsor', 'Womelsdorf', 'Wood', 'Woodbury', 'Woodland', 'Woodlyn', 'Worthington', 'Wrightsville', 'Wyalusing', 'Wyncote', 'Wyoming', 'Yeagertown', 'York Haven', 'York Springs', 'York', 'Youngstown', 'Youngsville', 'Youngwood', 'Zelienople', 'Zion Grove']


pennsylvania_counties = [
    "Adams County", "Allegheny County", "Armstrong County", "Beaver County",
    "Bedford County", "Berks County", "Blair County", "Bradford County",
    "Bucks County", "Butler County", "Cambria County", "Cameron County",
    "Carbon County", "Centre County", "Chester County", "Clarion County",
    "Clearfield County", "Clinton County", "Columbia County", "Crawford County",
    "Cumberland County", "Dauphin County", "Delaware County", "Elk County",
    "Erie County", "Fayette County", "Forest County", "Franklin County",
    "Fulton County", "Greene County", "Huntingdon County", "Indiana County",
    "Jefferson County", "Juniata County", "Lackawanna County", "Lancaster County",
    "Lawrence County", "Lebanon County", "Lehigh County", "Luzerne County",
    "Lycoming County", "McKean County", "Mercer County", "Mifflin County",
    "Monroe County", "Montgomery County", "Montour County", "Northampton County",
    "Northumberland County", "Perry County", "Philadelphia County", "Pike County",
    "Potter County", "Schuylkill County", "Snyder County", "Somerset County",
    "Sullivan County", "Susquehanna County", "Tioga County", "Union County",
    "Venango County", "Warren County", "Washington County", "Wayne County",
    "Westmoreland County", "Wyoming County", "York County"
]

print(cities)

import csv
import pandas as pd
from datetime import datetime, timedelta
import random

# Dummy data generator
def generate_dummy_data(num_rows):
    data = []
    for _ in range(num_rows):
        start_location = f"Location_{random.randint(1, 100)}"
        end_location = f"Location_{random.randint(1, 100)}"
        weather = random.choice(['Sunny', 'Rainy', 'Cloudy', 'Snowy'])
        distance = random.randint(5, 500)  # in km
        estimated_risk = random.choice(['Low', 'Medium', 'High'])
        expected_time = random.randint(60, 720)  # in minutes
        actual_trip_duration = expected_time + random.randint(-120, 120)  # in minutes, some variation
        departure_time = datetime.now().replace(microsecond=0, second=0, minute=0) + timedelta(days=random.randint(1, 30))
        delay_time = max(0, random.randint(-60, 120))  # in minutes, clamped to 0 if negative
        arrival_time = departure_time + timedelta(minutes=actual_trip_duration + delay_time)  # Calculate arrival time based on departure, duration, and delay
        data.append([start_location, end_location, weather, distance, estimated_risk, expected_time, actual_trip_duration, departure_time, arrival_time, delay_time])
    return data

# Function to write data to a CSV file
def write_to_csv(filename, data):
    with open(filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['start_location', 'end_location', 'weather', 'distance', 'estimated_risk', 'expected_time', 'actual_trip_duration', 'departure_time', 'arrival_time', 'delay_time'])
        writer.writerows(data)

# Function to display the CSV file
def display_csv(filename):
    data = pd.read_csv(filename)
    display(data)  # Use display() for Jupyter and Colab notebooks

# Generate and write dummy data to CSV
num_rows = 10  # Number of rows of dummy data to generate
dummy_data = generate_dummy_data(num_rows)
csv_filename = 'trip_data.csv'
write_to_csv(csv_filename, dummy_data)

# Display the contents of the CSV
display_csv(csv_filename)

import requests
from bs4 import BeautifulSoup

# URL of the page with the list of cities in Pennsylvania
url = "https://www.countryaah.com/alphabetical-list-of-cities-and-towns-in-pennsylvania/"

# Send a GET request to the page
response = requests.get(url)

# Parse the HTML content of the page
soup = BeautifulSoup(response.text, 'html.parser')

# Find the HTML elements that contain the cities' names
# This selector might need adjustment based on the page's structure
city_elements = soup.select('ul li a')

# Extract the text (city names) from the elements and print them
cities = [city.get_text() for city in city_elements if city.get_text()]

for city in cities:
    print(city)

!pip install scikit-learn



import openmeteo_requests

import requests_cache
import pandas as pd
from retry_requests import retry

# Setup the Open-Meteo API client with cache and retry on error
cache_session = requests_cache.CachedSession('.cache', expire_after = -1)
retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
openmeteo = openmeteo_requests.Client(session = retry_session)

def get_weather_shit(lat, lon, start_date, time):
# Make sure all required weather variables are listed here
# The order of variables in hourly or daily is important to assign them correctly below
  url = "https://archive-api.open-meteo.com/v1/archive"
  # start_date = f"{start_date}"
  params = {
    "latitude": lon,
    "longitude": lat,
    "start_date": start_date,
    "end_date": start_date,
    "daily": ["temperature_2m_max", "temperature_2m_min", "precipitation_sum", "rain_sum", "snowfall_sum", "wind_speed_10m_max", "wind_gusts_10m_max"],
    "temperature_unit": "fahrenheit",
    "wind_speed_unit": "mph",
    "timezone": "America/New_York"
  }
  responses = openmeteo.weather_api(url, params=params)

  # Process first location. Add a for-loop for multiple locations or weather models
  response = responses[0]
  # Process daily data. The order of variables needs to be the same as requested.
  daily = response.Daily()
  daily_temperature_2m_max = daily.Variables(0).ValuesAsNumpy()
  daily_temperature_2m_min = daily.Variables(1).ValuesAsNumpy()
  daily_precipitation_sum = daily.Variables(2).ValuesAsNumpy()
  daily_rain_sum = daily.Variables(3).ValuesAsNumpy()
  daily_snowfall_sum = daily.Variables(4).ValuesAsNumpy()
  daily_wind_speed_10m_max = daily.Variables(5).ValuesAsNumpy()
  daily_wind_gusts_10m_max = daily.Variables(6).ValuesAsNumpy()

  # print("AFTER NUMPY, lat, lon ", daily_temperature_2m_min, lat, lon)


  daily_data = {"date": pd.date_range(
    start = pd.to_datetime(daily.Time(), unit = "s"),
    end = pd.to_datetime(daily.TimeEnd(), unit = "s"),
    freq = pd.Timedelta(seconds = daily.Interval()),
    inclusive = "left"
  )}


  risk_factor = calculate_danger_score(calculate_normalized_danger_score_for_weather(F = daily_temperature_2m_min[0], P = daily_precipitation_sum[0]),
                         calculate_danger_score_for_time(expected_time = time))

  daily_data["temperature_2m_max"] = daily_temperature_2m_max
  daily_data["temperature_2m_min"] = daily_temperature_2m_min
  daily_data["precipitation_sum"] = daily_precipitation_sum
  daily_data["rain_sum"] = daily_rain_sum
  daily_data["snowfall_sum"] = daily_snowfall_sum
  daily_data["wind_speed_10m_max"] = daily_wind_speed_10m_max
  daily_data["wind_gusts_10m_max"] = daily_wind_gusts_10m_max
  daily_data["risk_factor"] = risk_factor

  # calculate_danger_score(calculate_normalized_danger_score_for_weather(F, P), calculate_danger_score_for_time(expected_time))


  daily_dataframe = pd.DataFrame(data = daily_data)
  return (daily_temperature_2m_max,daily_temperature_2m_min,daily_precipitation_sum,daily_rain_sum,daily_snowfall_sum,daily_wind_speed_10m_max,daily_wind_gusts_10m_max, risk_factor)

# get_weather_shit(40.44, -79.99, '2013-09-22', 1000)

import random
from datetime import date, timedelta
def generate_dates():
    # Start and end dates for the year 2020
    start_date = date(2010, 1, 1)
    end_date = date(2023, 12, 31)

    # Generate all dates in 2020
    delta = end_date - start_date
    all_dates = [start_date + timedelta(days=i) for i in range(delta.days + 1)]

    # Shuffle the list of dates to ensure randomness
    random.shuffle(all_dates)

    # Function to yield dates one by one
    def get_date():
        if all_dates:
            return (all_dates.pop()).strftime('%Y-%m-%d')
        else:
            return None  # or raise an exception, if you prefer

    return get_date

# Usage:
get_date = generate_dates()
get_date()

import requests
import numpy as np
import math
import csv

STARTING_CITES = [('40.440624,-79.995888','40.259590,-76.881866'),('40.259590,-76.881866','40.037876,-76.305511'),('41.211472,-79.380951', '41.022072,-78.438057'),('40.913395,-77.778336', '41.408970,-75.662415'),('41.003510,-76.457649', '40.984482,-75.197411'),('40.336922,-75.921959','39.952583,-75.165222')]




def generate_truck_route(start_city, end_city, geoapify_api_key):
    # Construct the URL for the GeoApify API request
    url = f"https://api.geoapify.com/v1/routing?waypoints={start_city}|{end_city}&mode=drive&vehicle=truck&apiKey={geoapify_api_key}"

    # Make the request
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the JSON response
        data = response.json()
        # Extract the coordinates from the 'features' key
        coordinates = data['features'][0]['geometry']['coordinates'][0]
        return coordinates
    else:
        print(f"Error: {response.status_code}")
        return None
geoapify_api_key = '5b3f9e39baf543ea87dcb6b682def2b1'
# start_city = '40.440624,-79.995888'
# end_city = '39.952583,-75.165222'
def truck_times(start_city, end_city, geoapify_api_key):
    url = f"https://api.geoapify.com/v1/routing?waypoints={start_city}|{end_city}&mode=drive&vehicle=truck&apiKey={geoapify_api_key}"
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        # Extract the legs and their respective times
        legs = data['features'][0]['properties']['legs']
        times = [leg['time'] for leg in legs]  # We accumulate the time for each leg
        # Extract the coordinates from the 'geometry' key
        # print(times)
        return times
    else:
        print(f"Error: {response.status_code}")
        return None

def reverse_geocode_coordinates(coordinates_list):
    city_coordinates_pairs = []
    cities_set = set()
    ordered_city_list = []
    reverse_geocode_cache = {}  # Cache to store reverse geocode results

    # Truncate coordinates to reduce the granularity
    truncated_coordinates = truncate_coordinates(coordinates_list, precision=1)

    for coordinate in truncated_coordinates:
        lon, lat = coordinate

        # Check if the coordinate is already in the cache
        if (lat, lon) in reverse_geocode_cache:
            city = reverse_geocode_cache[(lat, lon)]
        else:
            url = f"https://api.bigdatacloud.net/data/reverse-geocode-client?latitude={lat}&longitude={lon}&localityLanguage=en"
            response = requests.get(url)

            if response.status_code == 200:
                data = response.json()
                city = data.get('city', '')
                reverse_geocode_cache[(lat, lon)] = city  # Cache the result
            else:
                print(f"Error: {response.status_code}")
                continue

        if city and city not in cities_set:
            city_coordinates_pairs.append((city, coordinate))
            cities_set.add(city)
            ordered_city_list.append(city)

    return city_coordinates_pairs, ordered_city_list

def truncate_coordinates(coordinates, precision=1, decimal_places=5):
    # Truncate and format coordinates to the specified precision and decimal places
    truncated_and_formatted_coordinates = []

    for lat, lon in coordinates:
        # Round the coordinates to the specified precision
        rounded_lat = round(lat, precision)
        rounded_lon = round(lon, precision)

        # Format the rounded numbers to have a fixed number of decimal places
        formatted_lat = f"{rounded_lat:.{decimal_places}f}"
        formatted_lon = f"{rounded_lon:.{decimal_places}f}"

        truncated_and_formatted_coordinates.append((formatted_lat, formatted_lon))

    return truncated_and_formatted_coordinates

def get_cities_along_route(start_city, end_city, geoapify_api_key):
    # Get the truck route
    route_coordinates = generate_truck_route(start_city, end_city, geoapify_api_key)

    if route_coordinates:
        # Sample the coordinates if there are too many, to avoid hitting rate limits
        # sampled_coordinates = route_coordinates[::max(1, len(route_coordinates) // 10)]

        # Get the cities along the route
        cities_with_coordinate, cities_along_route = reverse_geocode_coordinates(route_coordinates)
        return cities_with_coordinate, cities_along_route
    else:
        return None, None

def create_city_pairs(coords_along_route, city_list):
    # Create pairs of consecutive cities
    return [(coords_along_route[i], coords_along_route[i + 1]) for i in range(len(coords_along_route) - 1)], [(city_list[i], city_list[i + 1]) for i in range(len(city_list) - 1)]

def add_times(pairs):
  final_list = []
  for item in pairs:
    (city1, city2) = item
    _, (coord1x, coord1y) = city1
    _, (coord2x, coord2y) = city2
    # print(coord1y, coord1x)
    # break
    new_time = truck_times(f"{coord1y},{coord1x}", f"{coord2y},{coord2x}", geoapify_api_key)
    if new_time:
      final_list.append((city1, city2, new_time[0] / 60))
  return final_list

def format_final_pairs(data):
    city_pairs = []
    for item in data:
        # Extract the city names and the time (which is in a list, so take the first element)
        city1 = item[0][1]
        city2 = item[][0]
        city_pairs.append((city1, city2))
    return city_pairs
def main_workflow(start_city, end_city, geoapify_api_key = '5b3f9e39baf543ea87dcb6b682def2b1', num_dates = 35):
  coords_along_route, cities_along_route = get_cities_along_route(start_city, end_city, geoapify_api_key)
  coord_pairs, city_pairs = create_city_pairs(coords_along_route, cities_along_route)
  coords_with_times = add_times(coord_pairs)
  final_res = []
  for i in range(num_dates):
    date = get_date()
    print(date)
    for (city1,(name, (lat, lon)), time) in coords_with_times:
      weather_stuff = get_weather_shit(lat, lon, str(date), time)

      final_res.append(format_row((city1,(name, (lat, lon)), weather_stuff, time, date)))
  return final_res

  # call calculate_normalized_danger_score_for_weather(F, P), then call calculate_danger_score(weather_danger_score, time_danger_score) and then calculate_danger_score_for_time(expected_time)

  # Something like this:

  # calculate_danger_score(calculate_normalized_danger_score_for_weather(F, P), calculate_danger_score_for_time(expected_time))


def format_row(row):
  city1, city2, weather_stuff, time, date = row
  xName, xcoord = city1
  yName, ycoord = city2
  daily_temperature_2m_max,daily_temperature_2m_min,daily_precipitation_sum,daily_rain_sum,daily_snowfall_sum,daily_wind_speed_10m_max,daily_wind_gusts_10m_max, risk_factor = weather_stuff

  return (xName, xcoord, yName, ycoord, daily_temperature_2m_max,daily_temperature_2m_min,daily_precipitation_sum,daily_rain_sum,daily_snowfall_sum,daily_wind_speed_10m_max,daily_wind_gusts_10m_max, time, risk_factor,date)

def main():
  header = ["start city name", "start city coordinates", "intermediate city name", "intermediate city coordinates", "daily_temperature_2m_max","daily_temperature_2m_min","daily_precipitation_sum","daily_rain_sum","daily_snowfall_sum","daily_wind_speed_10m_max","daily_wind_gusts_10m_max", "time(minutes)", "risk_factor", "date"]
    # Create a new CSV file and write the header
  for i, (start, dest) in enumerate(STARTING_CITES):
    with open(f"test{i+1}.csv", mode='a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(header)
        rows = main_workflow(start, dest, geoapify_api_key = '5b3f9e39baf543ea87dcb6b682def2b1')
        print("writing rows")
        writer = csv.writer(file)
        # Write the data rows
        writer.writerows(rows)
main()

pd.read_csv("/content/test1.csv")

!pip install openmeteo-requests
!pip install requests-cache retry-requests numpy pandas

"""# Dataloader/Dataclass"""



"""# Preprocessing / Optimizations"""



"""#Training"""

